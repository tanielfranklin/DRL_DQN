# my configuration file
state_shape: 3  # active joints
delta: 0.02 # incremental action step applied to the joints
RESTORE_AGENT: False  # Restore a trained agent
fitness: 0.005 # threshold value that defines the sucess of the task
renderize: False  # stop robot viewing
# to clip the gradients
max_grad_norm: 4000

#fitness parameters
sig_p: 1.0 # importance of position of end effector
sig_R: 0.1 # importance of pose of end effector

#Reset geral
reset_j1: [-1.74,1.7] #[-1.7,-1.] 
reset_j2: [-3,-0.06]
reset_j3: [0,3.75]

#Reward parameters
step_penalty: -30
collision_penalty: -100
success_rw: 1000
mag: 100 #magnification factor of reward

# Activate ceil obstacle
ceil: True

# set a seed
seed: 13
# Fill buffer with samples collected ramdomly from environment

#Maximum total_steps per episode
tmax: 300

comment: reinf  #comment to mark folder name
LOAD_MODEL: False
resume_folder: obs_10_12
model_resume: last  #among last, best (best rewards) and other (lowest TD loss)

batch_size: 64
total_steps: 5000 #epochs

# init Optimizer
lr_exp: -3 # 10**exp learning 
# setup some frequency for logging 
eval_freq: 100


