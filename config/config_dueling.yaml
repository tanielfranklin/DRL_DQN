# my configuration file
state_shape: 3  # active joints
delta: 0.02 # incremental action step applied to the joints
RESTORE_AGENT: False  # Restore a trained agent
fitness: 0.05 # threshold value that defines the sucess of the task

TRAIN: True  # Train or only simulate
renderize: False  # stop robot viewing
# to clip the gradients
max_grad_norm: 5000

#fitness parameters
sig_p: 1 # importance of position of end effector
sig_R: 0.1 # importance of pose of end effector
#j_lim=[(-1.74,1.76),(-3,-0.06),(0,3.75)]

#Reset geral
reset_j1: [-1.74,1.7] #[-1.7,-1.] 
reset_j2: [-3,-0.06]
reset_j3: [0,3.75]

# # proximo do alvo
# reset_j1: [-0.3,0.3] #[-1.7,-1.] 
# reset_j2: [-1,0.06]
# reset_j3: [0,3.75]

# env.reset_j1= [-0.3,0.3] #[-1.7,-1.] 
# env.reset_j2= [-1,0.06]
# env.reset_j3= [0,3.75]

#[1.4190038033490282e-15, -0.7299999999999993, 1.5000000000000062]
#Reward parameters
step_penalty: -10
collision_penalty: -100

#DQN layers
layer1: 256

#Dueling DQN
dueling_layers: [32,128,32] #or [64,256,64] hidden
# set a seed
seed: 13
# Fill buffer with samples collected ramdomly from environment

#MAximum total_steps per episode
tmax: 270
mag: 100 #magnification factor of reward
comment: new_2  #comment to mark folder name
LOAD_MODEL: True
resume_folder: new_2_3
model_resume: last  #among last, best (best rewards) and other (lowest TD loss)
NEW_BUFFER: False  # Restore a buffer
buffer_len: 30000
n_games_eval: 5 # number of games used to evaluate policy
n_steps_buffering: 270 # number of steps buffering after reset


timesteps_per_epoch: 1 #number of steps in the new policy appended in buffer
batch_size: 64
total_steps: 100000 #epochs

# init Optimizer
lr_exp: -4 # 10**exp learning 
# set exploration epsilon
percentage_of_total_steps: 0.5
percentage_of_total_steps_resume: 0.6
start_epsilon: 0.2  #high exploration
#start_epsilon = 0.1
end_epsilon: 0.05  #low exploration and max exploitation

# setup some frequency for logging and updating target network
loss_freq: 40
refresh_target_network_freq: 40
eval_freq: 2000


