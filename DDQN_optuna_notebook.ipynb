{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "import yaml\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import data_panda as rbt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import glob\n",
    "import joblib\n",
    "import os\n",
    "from optuna.trial import TrialState\n",
    "import logging\n",
    "import sys\n",
    "import optuna\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if device=='cpu':\n",
    "    print('gpu is not available')\n",
    "    print(device)\n",
    "    exit()\n",
    "\n",
    "# folder to load config file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = rbt.load_config(\"config_dueling.yaml\")\n",
    "\n",
    "def objective(trial):\n",
    "    state_shape = config['state_shape']\n",
    "    env = rbt.Panda_RL()\n",
    "    env.renderize = config['renderize']  # stop robot viewing\n",
    "    env.delta = config['delta']\n",
    "    agent = rbt.DQNAgent(state_shape, device, epsilon=1).to(device)\n",
    "\n",
    "    #init agent, target network and Optimizer\n",
    "    agent = rbt.DuelingDQNAgent(state_shape,device,config[\"dueling_layers\"], epsilon=1).to(device)\n",
    "    target_network = rbt.DuelingDQNAgent(state_shape,device, config[\"dueling_layers\"], epsilon=1).to(device)\n",
    "    target_network.load_state_dict(agent.state_dict())\n",
    "    optimizer = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "    RESTORE_AGENT = config['RESTORE_AGENT']  # Restore a trained agent\n",
    "    NEW_BUFFER = config['NEW_BUFFER']   # Restore a buffer\n",
    "    TRAIN = config['TRAIN']  # Train or only simulate\n",
    "    \n",
    "    \n",
    "\n",
    "    # agent.n_layer1=config['layer1']\n",
    "    # agent.n_layer2=config['layer1']*2\n",
    "\n",
    "    env.step_penalty=config['step_penalty']\n",
    "    env.collision_penalty=config['collision_penalty']\n",
    "    env.sig_p=config['sig_p']\n",
    "    env.sig_R=config['sig_R']\n",
    "\n",
    "\n",
    "    # set a seed\n",
    "    seed = config['seed']\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Fill buffer with samples collected ramdomly from environment\n",
    "    buffer_len = trial.suggest_int('Buffer len',1000,20000)\n",
    "     \n",
    "    exp_replay = rbt.PrioritizedReplayBuffer(buffer_len)\n",
    "    RES_DIR = rbt.set_res_dir(comment=config['comment'])\n",
    "    comment = \"\"\n",
    "    # monitor_tensorboard()\n",
    "    tb = SummaryWriter(log_dir=RES_DIR, comment=comment)\n",
    "\n",
    "    LOAD_MODEL=config['LOAD_MODEL']\n",
    "    if LOAD_MODEL:\n",
    "        folder=config['resume_folder']\n",
    "        agent.load_weights(folder,model=config['model_resume'])\n",
    "        percentage_of_total_steps=config['percentage_of_total_steps_resume']\n",
    "        print(f\"Loaded {folder} {config['model_resume']} \")\n",
    "        #print(f\"Restored  {folder}\")  \n",
    "\n",
    "\n",
    "    if NEW_BUFFER:\n",
    "        for i in trange(500,desc=\"Buffering\",ncols=70):\n",
    "\n",
    "            state = env.reset()\n",
    "            # Play 100 runs of experience with 100 steps and  stop if reach 10**4 samples\n",
    "            rbt.play_and_record(state, agent, env, exp_replay, n_steps=50)\n",
    "\n",
    "            if len(exp_replay) == buffer_len:\n",
    "                break\n",
    "        print(f\"New buffer with {len(exp_replay)} samples\")\n",
    "    else:\n",
    "        exp_replay = rbt.PrioritizedReplayBuffer(buffer_len)\n",
    "        exp_replay.load_buffer(config['resume_folder'])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    tmax = config['tmax']\n",
    "    env.reset_j1=config['reset_j1']\n",
    "    env.mag=config['mag']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # monitor_tensorboard()\n",
    "    tb = SummaryWriter(log_dir=RES_DIR, comment=comment)\n",
    "\n",
    "    percentage_of_total_steps = config['percentage_of_total_steps']\n",
    "\n",
    "\n",
    "\n",
    "    # setup some parameters for training\n",
    "\n",
    "    timesteps_per_epoch = config['timesteps_per_epoch']\n",
    "    #timesteps_per_epoch = trial.suggest_int('timesteps_per_epoch',1,3)\n",
    "    batch_size = config['batch_size']\n",
    "    total_steps = config['total_steps']\n",
    "    #total_steps = 10\n",
    "\n",
    "    # init Optimizer\n",
    "    lr_exp = config['lr_exp']\n",
    "    lr_exp=trial.suggest_int('lr',-4,-2)\n",
    "    lr=10**lr_exp\n",
    "    opt = torch.optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "    # set exploration epsilon\n",
    "    start_epsilon = config['start_epsilon']\n",
    "    #start_epsilon = 0.1\n",
    "    end_epsilon = config['end_epsilon']\n",
    "    eps_decay_final_step = percentage_of_total_steps*total_steps\n",
    "\n",
    "    # setup some frequency for logging and updating target network\n",
    "    loss_freq = config['loss_freq']\n",
    "    refresh_target_network_freq = trial.suggest_int('Refresh rate',100,300)\n",
    "    eval_freq = config['eval_freq']\n",
    "\n",
    "    # to clip the gradients\n",
    "    max_grad_norm = config['max_grad_norm']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    hyperparameters_train = {\"start_epsilon\": start_epsilon,\n",
    "                            \"end_epsilon\": end_epsilon,\n",
    "                            \"lr\": lr_exp,\n",
    "                            \"batch_size\": batch_size,\n",
    "                            \"total_steps\": total_steps,\n",
    "                            \"percentage_of_total_steps\": percentage_of_total_steps,\n",
    "                            \"refresh_target_network_freq\": refresh_target_network_freq,\n",
    "                            \"buffer_len\": buffer_len,\n",
    "                            \"tmax\": tmax\n",
    "                            #\"agent\": str(agent.network)\n",
    "                            }\n",
    "\n",
    "\n",
    "    def save_hyperparameter(dict, directory):\n",
    "        with open(directory+\"/hyperparameters.json\", \"w\") as outfile:\n",
    "            json.dump(dict, outfile)\n",
    "\n",
    "\n",
    "    # Start training\n",
    "    state = env.reset()\n",
    "    # tb.add_graph(agent.network, torch.tensor(\n",
    "    #     state, device=device, dtype=torch.float32))\n",
    "    save_hyperparameter(hyperparameters_train, RES_DIR)\n",
    "    loss_min = np.inf\n",
    "    rw_max=-np.inf\n",
    "    print(f\"buffer size = {len(exp_replay)} \")  \n",
    "    print(f\"Frequency evaluation = {eval_freq}\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "    for step in trange(total_steps + 1, desc=\"Training\", ncols=70):\n",
    "\n",
    "\n",
    "        # reduce exploration as we progress\n",
    "        agent.epsilon = rbt.epsilon_schedule(\n",
    "            start_epsilon, end_epsilon, step, eps_decay_final_step)\n",
    "\n",
    "        # take timesteps_per_epoch and update experience replay buffer\n",
    "        _, state = rbt.play_and_record(\n",
    "            state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # train by sampling batch_size of data from experience replay\n",
    "        states, actions, rewards, next_states, done_flags, weights, idxs = exp_replay.sample(\n",
    "            batch_size)\n",
    "        actions = [agent.get_action_index(i) for i in actions]\n",
    "\n",
    "        # loss = <compute TD loss>\n",
    "        opt.zero_grad()\n",
    "        loss = rbt.compute_td_loss_priority_replay(agent, target_network, exp_replay,\n",
    "                                                states, actions, rewards, next_states, done_flags, weights, idxs,\n",
    "                                                gamma=0.99,\n",
    "                                                device=device)\n",
    "        loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        if loss < loss_min:\n",
    "            torch.save(agent.state_dict(), RES_DIR+'/best-model-loss.pt')\n",
    "            loss_min=loss\n",
    "        tb.add_scalar(\"1/Epsilon\", agent.epsilon, step)\n",
    "        tb.add_scalar(\"1/TD Loss\", loss, step)\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            # Load agent weights into target_network\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            # eval the agent\n",
    "            assert not np.isnan(loss.cpu().detach().numpy())\n",
    "            #clear_output(True)        \n",
    "            m_reward,m_steps,m_collisions,m_successes,fit,_ = rbt.evaluate(env, agent, n_games=10,\n",
    "                                    greedy=True, t_max=tmax)\n",
    "            tb.add_scalar(\"1/Mean reward per episode\", m_reward, step)\n",
    "            tb.add_scalar(\"1/Mean of steps\", m_steps, step)\n",
    "            tb.add_scalar(\"2/Mean fitness reached\", fit, step)\n",
    "            tb.add_scalar(\"2/Mean of collisions\", m_collisions, step)\n",
    "            tb.add_scalar(\"2/Mean of successes\", m_successes, step)\n",
    "            #print(f\"Last mean reward = {m_reward}\")\n",
    " \n",
    "        if m_reward > rw_max:\n",
    "            torch.save(agent.state_dict(), RES_DIR+'/best-model-rw.pt')\n",
    "            rw_max=m_reward\n",
    "            \n",
    "        \n",
    "        #clear_output(True)\n",
    "    exp_replay.save_buffer(RES_DIR)\n",
    "    torch.save(agent.state_dict(), RES_DIR+'/last-model.pt')\n",
    "    tb.close()\n",
    "    rbt.eval_trained_models(env,agent,RES_DIR,device)\n",
    "    \n",
    "    return rw_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_optuna_experiment(study):\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-27 17:35:07,835]\u001b[0m A new study created in RDB with name: study_teste\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new study created in RDB with name: study_teste\n",
      "Current number of result directories: 5\n",
      "runs/teste_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Buffering:   0%|                              | 0/500 [00:00<?, ?it/s]/home/taniel/mambaforge/envs/RTB/lib/python3.10/site-packages/roboticstoolbox/robot/Link.py:1041: FutureWarning: base kwarg is deprecated, use pose instead\n",
      "  warn(\"base kwarg is deprecated, use pose instead\", FutureWarning)\n",
      "pybullet build time: Jul 21 2022 19:48:53\n",
      "Buffering:  19%|███▉                 | 94/500 [00:05<00:24, 16.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New buffer with 4702 samples\n",
      "buffer size = 4702 \n",
      "Frequency evaluation = 2000\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████| 100001/100001 [10:18<00:00, 161.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer and priorities Saved\n",
      "runs/teste_6/last-model.pt\n",
      "11976.304011692726 453.4 0.1 0.0 3.2291898809190953\n",
      "runs/teste_6/best-model-loss.pt\n",
      "27541.145886890903 477.5 0.05 0.0 2.5384263642128824\n",
      "runs/teste_6/best-model-rw.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-27 17:46:01,049]\u001b[0m Trial 0 finished with value: 51305.291787838665 and parameters: {'Buffer len': 4702, 'lr': -3, 'Refresh rate': 223}. Best is trial 0 with value: 51305.291787838665.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46420.27837721947 474.8 0.05 0.0 2.570781643868136\n",
      "m_reward,m_steps,m_collisions,m_successes,fit\n",
      "Trial 0 finished with value: 51305.291787838665 and parameters: {'Buffer len': 4702, 'lr': -3, 'Refresh rate': 223}. Best is trial 0 with value: 51305.291787838665.\n",
      "Current number of result directories: 6\n",
      "runs/teste_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Buffering:  10%|██                   | 48/500 [00:02<00:24, 18.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New buffer with 2446 samples\n",
      "buffer size = 2446 \n",
      "Frequency evaluation = 2000\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████| 100001/100001 [08:20<00:00, 199.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer and priorities Saved\n",
      "runs/teste_7/last-model.pt\n",
      "1261.1411313627386 65.6 1.0 0.0 2.9126755777787596\n",
      "runs/teste_7/best-model-loss.pt\n",
      "-6489.02303891498 75.8 1.0 0.0 3.79049956821493\n",
      "runs/teste_7/best-model-rw.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-27 17:54:35,109]\u001b[0m Trial 1 finished with value: 46209.13466826778 and parameters: {'Buffer len': 2446, 'lr': -2, 'Refresh rate': 249}. Best is trial 1 with value: 46209.13466826778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36043.20051427425 426.2 0.15 0.0 2.8288687423138947\n",
      "m_reward,m_steps,m_collisions,m_successes,fit\n",
      "Trial 1 finished with value: 46209.13466826778 and parameters: {'Buffer len': 2446, 'lr': -2, 'Refresh rate': 249}. Best is trial 1 with value: 46209.13466826778.\n",
      "Current number of result directories: 7\n",
      "runs/teste_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Buffering:  72%|██████████████▍     | 362/500 [00:18<00:06, 19.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New buffer with 18149 samples\n",
      "buffer size = 18149 \n",
      "Frequency evaluation = 2000\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████| 100001/100001 [10:40<00:00, 156.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer and priorities Saved\n",
      "runs/teste_8/last-model.pt\n",
      "36899.17723838846 432.4 0.15 0.0 2.6335973290679915\n",
      "runs/teste_8/best-model-loss.pt\n",
      "36717.381212163964 450.35 0.1 0.0 2.719322654055339\n",
      "runs/teste_8/best-model-rw.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-27 18:05:58,992]\u001b[0m Trial 2 finished with value: 47790.620073709775 and parameters: {'Buffer len': 18149, 'lr': -3, 'Refresh rate': 123}. Best is trial 1 with value: 46209.13466826778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35218.16564118344 378.4 0.25 0.0 2.7277807860797383\n",
      "m_reward,m_steps,m_collisions,m_successes,fit\n",
      "Trial 2 finished with value: 47790.620073709775 and parameters: {'Buffer len': 18149, 'lr': -3, 'Refresh rate': 123}. Best is trial 1 with value: 46209.13466826778.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study_name = \"study_teste\"  # Unique identifier of the study.\n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "study = optuna.create_study(study_name=study_name, storage=storage_name, direction=\"maximize\")\n",
    "\n",
    "\n",
    "study.optimize(objective, n_trials=3)\n",
    "# study = joblib.load('study_new_env1.pkl')\n",
    "# print(study.trials_dataframe())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer Loaded\n",
      "Replay Buffer Saved\n"
     ]
    }
   ],
   "source": [
    "# with open('runs/env1_after_opt_27/buffer.pickle', 'rb') as handle:\n",
    "#     buffer=pickle.load(handle)\n",
    "\n",
    "\n",
    "# with open('runs/env1_after_opt_27/priorities.pickle', 'rb') as handle:\n",
    "#     prio=pickle.load(handle)\n",
    "# print(\"Replay Buffer Loaded\")\n",
    "\n",
    "# with open('runs/env1_after_opt_27/buffer_3000.pickle', 'wb') as handle:\n",
    "#     pickle.dump(buffer[0:5000], handle)\n",
    "    \n",
    "# with open('runs/env1_after_opt_27/prio_3000.pickle', 'wb') as handle:\n",
    "#     pickle.dump(prio[0:5000], handle)\n",
    "# print(\"Replay Buffer Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=4)\n",
    "# joblib.dump(study, \"study_new_env1.pkl\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('RTB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea6380b41b4f676216da14f805c910367033db88baf08f4a008feb7da3e0a6b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
