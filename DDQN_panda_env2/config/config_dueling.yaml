# my configuration file
state_shape: 3
delta: 0.02
RESTORE_AGENT: False  # Restore a trained agent
fitness: 0.005

TRAIN: True  # Train or only simulate
renderize: False  # stop robot viewing
# to clip the gradients
max_grad_norm: 5000

#fitness parameters
sig_p: 1
sig_R: 0
reset_j1: [-1.7,1.7] #[-1.7,-1.] 
#Reward parameters
step_penalty: 0
collision_penalty: -100

#DQN layers
layer1: 256

#Dueling DQN
dueling_layers: [32,128,32] #or [64,256,64]
# set a seed
seed: 13
# Fill buffer with samples collected ramdomly from environment

#MAximum total_steps per episode
tmax: 500
mag: 100 #magnification factor of reward
comment: teste  #comment to mark folder name
LOAD_MODEL: True
resume_folder: teste
model_resume: last
NEW_BUFFER: False  # Restore a buffer
buffer_len: 3000


timesteps_per_epoch: 1
batch_size: 32
total_steps: 60000

# init Optimizer
lr: 0.01
# set exploration epsilon
percentage_of_total_steps: 0.6
percentage_of_total_steps_resume: 0.6
start_epsilon: 0.2
#start_epsilon = 0.1
end_epsilon: 0.05

# setup some frequency for logging and updating target network
loss_freq: 40
refresh_target_network_freq: 140
eval_freq: 2000


