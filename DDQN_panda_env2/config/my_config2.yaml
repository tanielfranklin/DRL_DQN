# my configuration file
state_shape: 3
delta: 0.02
RESTORE_AGENT: False  # Restore a trained agent

TRAIN: True  # Train or only simulate
renderize: False  # stop robot viewing
layer1: 256
# set a seed
seed: 13
# Fill buffer with samples collected ramdomly from environment

#MAximum total_steps per episode
tmax: 500
mag: 100 #magnification factor of reward
comment: 256_units
LOAD_MODEL: false
resume_folder: 256_units
model_resume: last
NEW_BUFFER: False  # Restore a buffer
buffer_len: 20000


timesteps_per_epoch: 1
batch_size: 32
total_steps: 100000

# init Optimizer
lr: 0.0001
# set exploration epsilon
percentage_of_total_steps: 0.65
percentage_of_total_steps_resume: 0.8
start_epsilon: 1
#start_epsilon = 0.1
end_epsilon: 0.05

# setup some frequency for logging and updating target network
loss_freq: 100
refresh_target_network_freq: 10
eval_freq: 2000
# to clip the gradients
max_grad_norm: 5000

#fitness parameters
sig_p: 1
sig_R: 0
reset_j1: [-1.7,1.7] #[-1.7,-1.] 
#Reward parameters
step_penalty: 0
collision_penalty: -100

