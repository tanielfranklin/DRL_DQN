{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "import yaml\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import data_panda as rbt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import glob\n",
    "import joblib\n",
    "import os\n",
    "from optuna.trial import TrialState\n",
    "import logging\n",
    "import sys\n",
    "import optuna\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if device=='cpu':\n",
    "    print('gpu is not available')\n",
    "    print(device)\n",
    "    exit()\n",
    "\n",
    "# folder to load config file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = rbt.load_config(\"config_dueling.yaml\")\n",
    "\n",
    "def objective(trial):\n",
    "    state_shape = config['state_shape']\n",
    "    env = rbt.Panda_RL()\n",
    "    env.renderize = config['renderize']  # stop robot viewing\n",
    "    env.delta = config['delta']\n",
    "    agent = rbt.DQNAgent(state_shape, device, epsilon=1).to(device)\n",
    "\n",
    "    #init agent, target network and Optimizer\n",
    "    agent = rbt.DuelingDQNAgent(state_shape,device,config[\"dueling_layers\"], epsilon=1).to(device)\n",
    "    target_network = rbt.DuelingDQNAgent(state_shape,device, config[\"dueling_layers\"], epsilon=1).to(device)\n",
    "    target_network.load_state_dict(agent.state_dict())\n",
    "    optimizer = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "    RESTORE_AGENT = config['RESTORE_AGENT']  # Restore a trained agent\n",
    "    NEW_BUFFER = config['NEW_BUFFER']   # Restore a buffer\n",
    "    TRAIN = config['TRAIN']  # Train or only simulate\n",
    "    \n",
    "    \n",
    "\n",
    "    # agent.n_layer1=config['layer1']\n",
    "    # agent.n_layer2=config['layer1']*2\n",
    "\n",
    "    env.step_penalty=config['step_penalty']\n",
    "    env.collision_penalty=config['collision_penalty']\n",
    "    env.sig_p=config['sig_p']\n",
    "    env.sig_R=config['sig_R']\n",
    "\n",
    "\n",
    "    # set a seed\n",
    "    seed = config['seed']\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Fill buffer with samples collected ramdomly from environment\n",
    "    buffer_len = trial.suggest_int('Buffer len',1000,20000)\n",
    "     \n",
    "    exp_replay = rbt.PrioritizedReplayBuffer(buffer_len)\n",
    "    RES_DIR = rbt.set_res_dir(comment=config['comment'])\n",
    "    comment = \"\"\n",
    "    # monitor_tensorboard()\n",
    "    tb = SummaryWriter(log_dir=RES_DIR, comment=comment)\n",
    "\n",
    "    LOAD_MODEL=config['LOAD_MODEL']\n",
    "    if LOAD_MODEL:\n",
    "        folder=config['resume_folder']\n",
    "        agent.load_weights(folder,model=config['model_resume'])\n",
    "        percentage_of_total_steps=config['percentage_of_total_steps_resume']\n",
    "        print(f\"Loaded {folder} {config['model_resume']} \")\n",
    "        #print(f\"Restored  {folder}\")  \n",
    "\n",
    "\n",
    "    if NEW_BUFFER:\n",
    "        for i in trange(500,desc=\"Buffering\",ncols=70):\n",
    "\n",
    "            state = env.reset()\n",
    "            # Play 100 runs of experience with 100 steps and  stop if reach 10**4 samples\n",
    "            rbt.play_and_record(state, agent, env, exp_replay, n_steps=50)\n",
    "\n",
    "            if len(exp_replay) == buffer_len:\n",
    "                break\n",
    "        print(f\"New buffer with {len(exp_replay)} samples\")\n",
    "    else:\n",
    "        exp_replay = rbt.PrioritizedReplayBuffer(buffer_len)\n",
    "        exp_replay.load_buffer(config['resume_folder'])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    tmax = config['tmax']\n",
    "    env.reset_j1=config['reset_j1']\n",
    "    env.mag=config['mag']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # monitor_tensorboard()\n",
    "    tb = SummaryWriter(log_dir=RES_DIR, comment=comment)\n",
    "\n",
    "    percentage_of_total_steps = config['percentage_of_total_steps']\n",
    "\n",
    "\n",
    "\n",
    "    # setup some parameters for training\n",
    "\n",
    "    timesteps_per_epoch = config['timesteps_per_epoch']\n",
    "    timesteps_per_epoch = trial.suggest_int('timesteps_per_epoch',1,3)\n",
    "    batch_size = config['batch_size']\n",
    "    total_steps = config['total_steps']\n",
    "    #total_steps = 10\n",
    "\n",
    "    # init Optimizer\n",
    "    lr = config['lr']\n",
    "    lr=trial.suggest_int('lr',0.0001,0.001)\n",
    "    opt = torch.optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "    # set exploration epsilon\n",
    "    start_epsilon = config['start_epsilon']\n",
    "    #start_epsilon = 0.1\n",
    "    end_epsilon = config['end_epsilon']\n",
    "    eps_decay_final_step = percentage_of_total_steps*total_steps\n",
    "\n",
    "    # setup some frequency for logging and updating target network\n",
    "    loss_freq = config['loss_freq']\n",
    "    refresh_target_network_freq = trial.suggest_int('Refresh rate',5,200)\n",
    "    eval_freq = config['eval_freq']\n",
    "\n",
    "    # to clip the gradients\n",
    "    max_grad_norm = config['max_grad_norm']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    hyperparameters_train = {\"start_epsilon\": start_epsilon,\n",
    "                            \"end_epsilon\": end_epsilon,\n",
    "                            \"lr\": lr,\n",
    "                            \"batch_size\": batch_size,\n",
    "                            \"total_steps\": total_steps,\n",
    "                            \"percentage_of_total_steps\": percentage_of_total_steps,\n",
    "                            \"refresh_target_network_freq\": refresh_target_network_freq,\n",
    "                            \"buffer_len\": buffer_len,\n",
    "                            \"tmax\": tmax\n",
    "                            #\"agent\": str(agent.network)\n",
    "                            }\n",
    "\n",
    "\n",
    "    def save_hyperparameter(dict, directory):\n",
    "        with open(directory+\"/hyperparameters.json\", \"w\") as outfile:\n",
    "            json.dump(dict, outfile)\n",
    "\n",
    "\n",
    "    # Start training\n",
    "    state = env.reset()\n",
    "    # tb.add_graph(agent.network, torch.tensor(\n",
    "    #     state, device=device, dtype=torch.float32))\n",
    "    save_hyperparameter(hyperparameters_train, RES_DIR)\n",
    "    loss_min = np.inf\n",
    "    rw_max=-np.inf\n",
    "    print(f\"buffer size = {len(exp_replay)} \")  \n",
    "    print(f\"Frequency evaluation = {eval_freq}\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "    for step in trange(total_steps + 1, desc=\"Training\", ncols=70):\n",
    "\n",
    "\n",
    "        # reduce exploration as we progress\n",
    "        agent.epsilon = rbt.epsilon_schedule(\n",
    "            start_epsilon, end_epsilon, step, eps_decay_final_step)\n",
    "\n",
    "        # take timesteps_per_epoch and update experience replay buffer\n",
    "        _, state = rbt.play_and_record(\n",
    "            state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # train by sampling batch_size of data from experience replay\n",
    "        states, actions, rewards, next_states, done_flags, weights, idxs = exp_replay.sample(\n",
    "            batch_size)\n",
    "        actions = [agent.get_action_index(i) for i in actions]\n",
    "\n",
    "        # loss = <compute TD loss>\n",
    "        opt.zero_grad()\n",
    "        loss = rbt.compute_td_loss_priority_replay(agent, target_network, exp_replay,\n",
    "                                                states, actions, rewards, next_states, done_flags, weights, idxs,\n",
    "                                                gamma=0.99,\n",
    "                                                device=device)\n",
    "        loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        if loss < loss_min:\n",
    "            torch.save(agent.state_dict(), RES_DIR+'/best-model-loss.pt')\n",
    "            loss_min=loss\n",
    "        tb.add_scalar(\"1/Epsilon\", agent.epsilon, step)\n",
    "        tb.add_scalar(\"1/TD Loss\", loss, step)\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            # Load agent weights into target_network\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            # eval the agent\n",
    "            assert not np.isnan(loss.cpu().detach().numpy())\n",
    "            #clear_output(True)        \n",
    "            m_reward,m_steps,m_collisions,m_successes,fit,_ = rbt.evaluate(env, agent, n_games=10,\n",
    "                                    greedy=True, t_max=tmax)\n",
    "            tb.add_scalar(\"1/Mean reward per episode\", m_reward, step)\n",
    "            tb.add_scalar(\"1/Mean of steps\", m_steps, step)\n",
    "            tb.add_scalar(\"2/Mean fitness reached\", fit, step)\n",
    "            tb.add_scalar(\"2/Mean of collisions\", m_collisions, step)\n",
    "            tb.add_scalar(\"2/Mean of successes\", m_successes, step)\n",
    "            #print(f\"Last mean reward = {m_reward}\")\n",
    " \n",
    "        if m_reward > rw_max:\n",
    "            torch.save(agent.state_dict(), RES_DIR+'/best-model-rw.pt')\n",
    "            rw_max=m_reward\n",
    "            \n",
    "        \n",
    "        #clear_output(True)\n",
    "    exp_replay.save_buffer(RES_DIR)\n",
    "    torch.save(agent.state_dict(), RES_DIR+'/last-model.pt')\n",
    "    tb.close()\n",
    "    rbt.eval_trained_models(env,agent,RES_DIR,device)\n",
    "    \n",
    "    return rw_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    number        value             datetime_start          datetime_complete  \\\n",
      "0        0  2792.213854 2022-10-26 21:02:36.133633 2022-10-26 21:11:25.414740   \n",
      "1        1  7008.531532 2022-10-26 21:11:25.415083 2022-10-26 21:18:11.399158   \n",
      "2        2  7226.427770 2022-10-26 21:18:11.399542 2022-10-26 21:24:34.424368   \n",
      "3        3  6517.168128 2022-10-26 21:24:34.424926 2022-10-26 21:32:49.237115   \n",
      "4        4  7557.297760 2022-10-26 21:32:49.237485 2022-10-26 21:40:02.260197   \n",
      "5        5  7008.531501 2022-10-26 21:40:02.260509 2022-10-26 21:49:13.999890   \n",
      "6        6  6568.330821 2022-10-26 21:49:14.000318 2022-10-26 21:57:15.065982   \n",
      "7        7  6567.352030 2022-10-26 21:57:15.066687 2022-10-26 22:05:07.605093   \n",
      "8        8  6517.168082 2022-10-26 22:05:07.605724 2022-10-26 22:11:45.118054   \n",
      "9        9  6643.221719 2022-10-26 22:11:45.118376 2022-10-26 22:20:59.123162   \n",
      "10      10  7175.216007 2022-10-26 22:20:59.123743 2022-10-26 22:28:27.201265   \n",
      "11      11  7226.427779 2022-10-26 22:28:27.201610 2022-10-26 22:35:31.665183   \n",
      "12      12  6564.699908 2022-10-26 22:35:31.665531 2022-10-26 22:42:36.537357   \n",
      "13      13  7765.304419 2022-10-26 22:42:36.537687 2022-10-26 22:49:42.009904   \n",
      "14      14  7557.298081 2022-10-26 22:49:42.010291 2022-10-26 22:57:08.595350   \n",
      "15      15  7226.427791 2022-10-26 22:57:08.595736 2022-10-26 23:05:57.549968   \n",
      "16      16  6567.352031 2022-10-26 23:05:57.550301 2022-10-26 23:12:56.784632   \n",
      "17      17  6592.208195 2022-10-26 23:12:56.784999 2022-10-26 23:23:07.152486   \n",
      "18      18  6143.118645 2022-10-26 23:23:07.152808 2022-10-26 23:30:02.563441   \n",
      "19      19  7008.531500 2022-10-26 23:30:02.563797 2022-10-26 23:38:19.548942   \n",
      "\n",
      "                 duration  params_Buffer len  params_Refresh rate  params_lr  \\\n",
      "0  0 days 00:08:49.281107              17325                  101          0   \n",
      "1  0 days 00:06:45.984075               5329                   59          0   \n",
      "2  0 days 00:06:23.024826               4359                    8          0   \n",
      "3  0 days 00:08:14.812189               9972                   11          0   \n",
      "4  0 days 00:07:13.022712              18701                  149          0   \n",
      "5  0 days 00:09:11.739381               1809                   88          0   \n",
      "6  0 days 00:08:01.065664               5452                   28          0   \n",
      "7  0 days 00:07:52.538406               4551                  118          0   \n",
      "8  0 days 00:06:37.512330               7988                  167          0   \n",
      "9  0 days 00:09:14.004786               4695                   18          0   \n",
      "10 0 days 00:07:28.077522              19941                  190          0   \n",
      "11 0 days 00:07:04.463573              14383                  143          0   \n",
      "12 0 days 00:07:04.871826              14491                  146          0   \n",
      "13 0 days 00:07:05.472217              14262                  131          0   \n",
      "14 0 days 00:07:26.585059              14208                  131          0   \n",
      "15 0 days 00:08:48.954232              13861                  120          0   \n",
      "16 0 days 00:06:59.234331              12073                   76          0   \n",
      "17 0 days 00:10:10.367487              16048                  196          0   \n",
      "18 0 days 00:06:55.410633              11145                  127          0   \n",
      "19 0 days 00:08:16.985145               8843                  168          0   \n",
      "\n",
      "    params_timesteps_per_epoch     state  \n",
      "0                            2  COMPLETE  \n",
      "1                            1  COMPLETE  \n",
      "2                            1  COMPLETE  \n",
      "3                            2  COMPLETE  \n",
      "4                            1  COMPLETE  \n",
      "5                            3  COMPLETE  \n",
      "6                            2  COMPLETE  \n",
      "7                            2  COMPLETE  \n",
      "8                            1  COMPLETE  \n",
      "9                            3  COMPLETE  \n",
      "10                           1  COMPLETE  \n",
      "11                           1  COMPLETE  \n",
      "12                           1  COMPLETE  \n",
      "13                           1  COMPLETE  \n",
      "14                           1  COMPLETE  \n",
      "15                           2  COMPLETE  \n",
      "16                           1  COMPLETE  \n",
      "17                           3  COMPLETE  \n",
      "18                           1  COMPLETE  \n",
      "19                           2  COMPLETE  \n"
     ]
    }
   ],
   "source": [
    "study = joblib.load('study_new_env1.pkl')\n",
    "print(study.trials_dataframe())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  20\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  20\n",
      "Best trial:\n",
      "  Value:  7765.30441876389\n",
      "  Params: \n",
      "    Buffer len: 14262\n",
      "    timesteps_per_epoch: 1\n",
      "    lr: 0\n",
      "    Refresh rate: 131\n"
     ]
    }
   ],
   "source": [
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer Loaded\n",
      "Replay Buffer Saved\n"
     ]
    }
   ],
   "source": [
    "with open('runs/env1_after_opt_27/buffer.pickle', 'rb') as handle:\n",
    "    buffer=pickle.load(handle)\n",
    "\n",
    "\n",
    "with open('runs/env1_after_opt_27/priorities.pickle', 'rb') as handle:\n",
    "    prio=pickle.load(handle)\n",
    "print(\"Replay Buffer Loaded\")\n",
    "\n",
    "with open('runs/env1_after_opt_27/buffer_3000.pickle', 'wb') as handle:\n",
    "    pickle.dump(buffer[0:5000], handle)\n",
    "    \n",
    "with open('runs/env1_after_opt_27/prio_3000.pickle', 'wb') as handle:\n",
    "    pickle.dump(prio[0:5000], handle)\n",
    "print(\"Replay Buffer Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=4)\n",
    "# joblib.dump(study, \"study_new_env1.pkl\")\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('RTB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea6380b41b4f676216da14f805c910367033db88baf08f4a008feb7da3e0a6b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
